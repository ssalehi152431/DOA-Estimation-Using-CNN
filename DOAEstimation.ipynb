{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDtDOAAHPWRN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XT_H7hOuPvAn"
      },
      "outputs": [],
      "source": [
        "# Data Preprocessing\n",
        "# Scale data to [0, 255] for consistency with RGB image channels\n",
        "def scale_data(data):\n",
        "    scaled_data = (data - np.min(data)) / (np.max(data) - np.min(data)) * 255\n",
        "    return scaled_data.astype(np.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezxdWedYP2-4",
        "outputId": "4741828c-91a4-4d10-95d5-3fb43129d14b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Data Preprocessing\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load datasets from Google Drive\n",
        "narrow_dataset_path = '/content/drive/MyDrive/ASP_Project/Data/narrow_grid_dataset.mat'\n",
        "wide_dataset_path = '/content/drive/MyDrive/ASP_Project/Data/wide_grid_dataset.mat'\n",
        "\n",
        "data_narrow = scipy.io.loadmat(narrow_dataset_path)\n",
        "data_wide = scipy.io.loadmat(wide_dataset_path)\n",
        "\n",
        "# Extract data and labels\n",
        "dataset_narrow = data_narrow['dataset_narrow']\n",
        "labels_narrow = data_narrow['labels_narrow']\n",
        "\n",
        "dataset_wide = data_wide['dataset_wide']\n",
        "labels_wide = data_wide['labels_wide']\n",
        "\n",
        "# Scale Data\n",
        "dataset_narrow_scaled = scale_data(dataset_narrow)\n",
        "dataset_wide_scaled = scale_data(dataset_wide)\n",
        "\n",
        "# Split data into training and testing subsets (90% training, 10% testing)\n",
        "x_train_narrow, x_test_narrow, y_train_narrow, y_test_narrow = train_test_split(dataset_narrow_scaled, labels_narrow, test_size=0.1, random_state=42)\n",
        "x_train_wide, x_test_wide, y_train_wide, y_test_wide = train_test_split(dataset_wide_scaled, labels_wide, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2r67f0kQS8p"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNCptWTGQury"
      },
      "outputs": [],
      "source": [
        "# Build Model\n",
        "def build_model():\n",
        "    inputs = tf.keras.Input(shape=(IMG_SIZE_Height, IMG_SIZE_Width, 3))\n",
        "\n",
        "    # Convolutional Layers with Batch Normalization and ReLU activation\n",
        "    x = layers.Conv2D(filters=256, kernel_size=3, activation='relu', padding='same')(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv2D(filters=256, kernel_size=2, activation='relu', padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv2D(filters=256, kernel_size=2, activation='relu', padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv2D(filters=256, kernel_size=2, activation='relu', padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # Flatten the output from the convolutional layers\n",
        "    x = layers.Flatten()(x)\n",
        "\n",
        "    # Fully Connected Layers\n",
        "    x = layers.Dense(4096, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Dense(2048, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Dense(1024, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "\n",
        "    # Output layer with Sigmoid activation for binary labels\n",
        "    outputs = layers.Dense(labels_narrow.shape[1], activation='sigmoid')(x)\n",
        "\n",
        "    model = models.Model(inputs, outputs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_AedkPPQ8ot"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE_Height, IMG_SIZE_Width = dataset_narrow.shape[1], dataset_narrow.shape[2]\n",
        "\n",
        "model = build_model()\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "collapsed": true,
        "id": "W5i1qjSyRRRs",
        "outputId": "76efa627-b3c4-446a-d457-4e6e88dfe32c"
      },
      "outputs": [],
      "source": [
        "# Train model\n",
        "# Train the model for the narrow dataset\n",
        "history_narrow = model.fit(x_train_narrow, y_train_narrow, epochs=200, batch_size=32, validation_data=(x_test_narrow, y_test_narrow))\n",
        "\n",
        "# Train the model for the wide dataset\n",
        "history_wide = model.fit(x_train_wide, y_train_wide, epochs=200, batch_size=32, validation_data=(x_test_wide, y_test_wide))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjCM9LTpRZiH"
      },
      "outputs": [],
      "source": [
        "# Noise addition\n",
        "def add_noise(data, snr_db):\n",
        "    signal_power = np.mean(data ** 2)\n",
        "    snr_linear = 10 ** (snr_db / 10)\n",
        "    noise_power = signal_power / snr_linear\n",
        "    noise = np.sqrt(noise_power) * np.random.randn(*data.shape)\n",
        "    return data + noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tpLb7tDRuCC"
      },
      "outputs": [],
      "source": [
        "# Model evaluation\n",
        "snrs = [0, -5, -10, -15, -20]\n",
        "\n",
        "# Evaluate the model for different SNR values and visualize the results\n",
        "results = {}\n",
        "for snr in snrs:\n",
        "    noisy_data_narrow = add_noise(x_test_narrow, snr)\n",
        "    predictions = model.predict(noisy_data_narrow)\n",
        "    results[snr] = predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHG_oJtgR7Yv"
      },
      "outputs": [],
      "source": [
        "# Generating results and plots\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for snr, predictions in results.items():\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    ax[0].plot(y_test_narrow[:, 0], label='Real $\\theta_1$', marker='^')\n",
        "    ax[0].plot(predictions[:, 0], label='Predicted $\\theta_1$', linestyle='--', marker='o')\n",
        "    ax[1].plot(y_test_narrow[:, 1], label='Real $\\theta_2$', marker='^')\n",
        "    ax[1].plot(predictions[:, 1], label='Predicted $\\theta_2$', linestyle='--', marker='o')\n",
        "    ax[0].set_title(f'DOA Estimation Performance for SNR = {snr}')\n",
        "    ax[1].set_title(f'DOA Estimation Error for SNR = {snr}')\n",
        "    ax[0].legend()\n",
        "    ax[1].legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcMs6YOwSJw6"
      },
      "outputs": [],
      "source": [
        "# Calculating RMSE and plotting\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "rmse_values = {}\n",
        "for snr, predictions in results.items():\n",
        "    rmse_values[snr] = np.sqrt(mean_squared_error(y_test_narrow, predictions))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(snrs, [rmse_values[snr] for snr in snrs], marker='o')\n",
        "plt.xlabel('SNR (dB)')\n",
        "plt.ylabel('RMSE (Degrees)')\n",
        "plt.title('RMSE for Different SNRs')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgyOJc19QV2Z"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
